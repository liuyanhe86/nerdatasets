{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17c7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, dev_dir, test_dir = './supervised/supervised/train.txt', './supervised/supervised/dev.txt', './supervised/supervised/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67af3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131767\n",
      "18824\n",
      "37648\n",
      "188239\n"
     ]
    }
   ],
   "source": [
    "train_sentences = []\n",
    "raw_lines = []\n",
    "with open(train_dir, encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "raw_lines[-1] = raw_lines[-1] + '\\n'\n",
    "raw_lines.extend(['\\n'])\n",
    "raw_lines = [line.strip() for line in raw_lines]\n",
    "\n",
    "beg, end = 0, 0\n",
    "while end < len(raw_lines):\n",
    "    if raw_lines[end]:\n",
    "        end += 1\n",
    "    else:\n",
    "        sentence = raw_lines[beg:end]\n",
    "        train_sentences.append(sentence)\n",
    "        end += 1\n",
    "        beg = end\n",
    "print(len(train_sentences))\n",
    "\n",
    "valid_sentences = []\n",
    "raw_lines = []\n",
    "with open(dev_dir, encoding='utf-8') as f:\n",
    "    raw_lines = f.readlines()\n",
    "raw_lines[-1] = raw_lines[-1] + '\\n'\n",
    "raw_lines.extend(['\\n'])\n",
    "raw_lines = [line.strip() for line in raw_lines]\n",
    "\n",
    "beg, end = 0, 0\n",
    "while end < len(raw_lines):\n",
    "    if raw_lines[end]:\n",
    "        end += 1\n",
    "    else:\n",
    "        sentence = raw_lines[beg:end]\n",
    "        valid_sentences.append(sentence)\n",
    "        end += 1\n",
    "        beg = end\n",
    "print(len(valid_sentences))\n",
    "\n",
    "test_sentences = []\n",
    "with open(test_dir, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "lines[-1] = lines[-1] + '\\n'\n",
    "lines.extend(['\\n'])\n",
    "lines = [line.strip() for line in lines]\n",
    "beg, end = 0, 0\n",
    "while end < len(lines):\n",
    "    if lines[end]:\n",
    "        end += 1\n",
    "    else:\n",
    "        sentence = lines[beg:end]\n",
    "        test_sentences.append(sentence)\n",
    "        end += 1\n",
    "        beg = end\n",
    "print(len(test_sentences))\n",
    "print(len(train_sentences) + len(valid_sentences) + len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed136d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113891\n",
      "['.\\tO']\n",
      "['.\\tO']\n",
      "['.\\tO']\n",
      "['.\\tO']\n",
      "16343\n",
      "32565\n",
      "162799\n"
     ]
    }
   ],
   "source": [
    "washed_train_sentences = []\n",
    "for sentence in train_sentences:\n",
    "    if len(sentence) >= 5:\n",
    "        for i in range(len(sentence)):\n",
    "            word_tag = sentence[i]\n",
    "            tokens = word_tag.split('\\t')\n",
    "            if tokens[1] != 'O':\n",
    "                if '/' in tokens[1]:\n",
    "                    tokens[1] = tokens[1].replace('/', ',')\n",
    "                    sentence[i] = '\\t'.join(tokens)\n",
    "        O_count = sum([1 if wt.endswith('\\tO') else 0 for wt in sentence])\n",
    "        if O_count != 0 and O_count != len(sentence):\n",
    "            washed_train_sentences.append(sentence)\n",
    "print(len(washed_train_sentences))\n",
    "\n",
    "washed_valid_sentences = []\n",
    "for sentence in valid_sentences:\n",
    "    if len(sentence) >= 5:\n",
    "        for i in range(len(sentence)):\n",
    "            word_tag = sentence[i]\n",
    "            tokens = word_tag.split('\\t')\n",
    "            if tokens[1] != 'O':\n",
    "                if '/' in tokens[1]:\n",
    "                    tokens[1] = tokens[1].replace('/', ',')\n",
    "                    sentence[i] = '\\t'.join(tokens)\n",
    "        O_count = sum([1 if wt.endswith('\\tO') else 0 for wt in sentence])\n",
    "        if O_count != 0 and O_count != len(sentence):\n",
    "            washed_valid_sentences.append(sentence)\n",
    "    else:\n",
    "        print(sentence)\n",
    "print(len(washed_valid_sentences))\n",
    "\n",
    "washed_test_sentences = []\n",
    "for sentence in test_sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        word_tag = sentence[i]\n",
    "        tokens = word_tag.split('\\t')\n",
    "        if tokens[1] != 'O':\n",
    "            if '/' in tokens[1]:\n",
    "                tokens[1] = tokens[1].replace('/', ',')\n",
    "                sentence[i] = '\\t'.join(tokens)\n",
    "    O_count = sum([1 if wt.endswith('\\tO') else 0 for wt in sentence])\n",
    "    if O_count != len(sentence):\n",
    "        washed_test_sentences.append(sentence)\n",
    "print(len(washed_test_sentences))\n",
    "print(len(washed_train_sentences) + len(washed_valid_sentences) + len(washed_test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba08f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154387\n",
      "fine-grained classes: {'art-music', 'organization-religion', 'person-artist,author', 'event-other', 'other-astronomything', 'product-food', 'organization-education', 'product-ship', 'person-other', 'location-road,railway,highway,transit', 'person-director', 'organization-other', 'art-broadcastprogram', 'building-airport', 'art-painting', 'other-chemicalthing', 'product-other', 'other-medical', 'building-library', 'other-currency', 'building-other', 'building-theater', 'other-biologything', 'product-weapon', 'person-soldier', 'organization-politicalparty', 'product-software', 'product-car', 'product-airplane', 'location-park', 'person-scholar', 'person-athlete', 'other-language', 'other-god', 'organization-showorganization', 'organization-sportsteam', 'event-protest', 'event-disaster', 'person-actor', 'location-island', 'organization-company', 'other-livingthing', 'other-disease', 'building-hotel', 'art-other', 'organization-government,governmentagency', 'event-attack,battle,war,militaryconflict', 'location-bodiesofwater', 'event-sportsevent', 'building-restaurant', 'person-politician', 'event-election', 'building-sportsfacility', 'location-GPE', 'location-mountain', 'organization-media,newspaper', 'product-game', 'art-film', 'location-other', 'art-writtenart', 'other-award', 'organization-sportsleague', 'other-educationaldegree', 'other-law', 'product-train', 'building-hospital'}\n"
     ]
    }
   ],
   "source": [
    "all_sentences = set(['\\n'.join(s) for s in washed_train_sentences] + ['\\n'.join(s) for s in washed_valid_sentences] + ['\\n'.join(s) for s in washed_test_sentences])\n",
    "print(len(all_sentences))\n",
    "fine_classes = set()\n",
    "for sentence in all_sentences:\n",
    "    wts = sentence.split('\\n')\n",
    "    for i in range(len(wts)):\n",
    "        word_tag = wts[i]\n",
    "        tokens = word_tag.split('\\t')\n",
    "        if tokens[1] != 'O':\n",
    "            if '/' in tokens[1]:\n",
    "                tokens[1] = tokens[1].replace('/', ',')\n",
    "                wts[i] = '\\t'.join(tokens)\n",
    "            fine_classes.add(tokens[1])\n",
    "print(f'fine-grained classes: {fine_classes}')\n",
    "fine_dict = {}\n",
    "\n",
    "for fine_class in fine_classes:\n",
    "    sentences = []\n",
    "    for sentence in all_sentences:\n",
    "        wts = sentence.split('\\n')\n",
    "        for word_tag in wts:\n",
    "            tag = word_tag.split('\\t')[1]\n",
    "            if tag == fine_class:\n",
    "                sentences.append(sentence)\n",
    "                break\n",
    "    fine_dict[fine_class] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "tag_count = {}\n",
    "for fc in fine_dict:\n",
    "    tag_count[fc] = len(fine_dict[fc])\n",
    "    count += len(fine_dict[fc])\n",
    "print(count)\n",
    "print(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_samples = set()\n",
    "train_samples_count = {}\n",
    "for tag in fine_classes:\n",
    "    samples = fine_dict[tag]\n",
    "    while train_samples_count.get(tag, 0) < 0.7 * tag_count[tag]:\n",
    "        sample = random.choice(samples)\n",
    "        if sample not in train_samples:\n",
    "            satisfied = True\n",
    "            other_tags = set([wt.split('\\t')[1] for wt in sample.split('\\n')])\n",
    "            for other_tag in other_tags:\n",
    "                if other_tag != 'O' and other_tag != tag:\n",
    "                    if train_samples_count.get(other_tag, 0) + 1 > 0.7 * tag_count[other_tag]:\n",
    "                        satisfied = False\n",
    "                        break\n",
    "            if satisfied:\n",
    "                train_samples.add(sample)\n",
    "                train_samples_count[tag] = train_samples_count.get(tag, 0) + 1\n",
    "                for other_tag in other_tags:\n",
    "                    if other_tag != 'O' and other_tag != tag:\n",
    "                        train_samples_count[other_tag] = train_samples_count.get(other_tag, 0) + 1\n",
    "print(len(train_samples))\n",
    "print(train_samples_count)\n",
    "print(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in train_samples_count:\n",
    "    print(tag, ':', train_samples_count[tag] / tag_count[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafa187",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_samples = set()\n",
    "valid_samples_count = {}\n",
    "for tag in fine_classes:\n",
    "    samples = fine_dict[tag]\n",
    "    while valid_samples_count.get(tag, 0) < 0.1 * tag_count[tag]:\n",
    "        sample = random.choice(samples)\n",
    "        if sample not in train_samples and sample not in valid_samples:\n",
    "            satisfied = True\n",
    "            other_tags = set([wt.split('\\t')[1] for wt in sample.split('\\n')])\n",
    "            for other_tag in other_tags:\n",
    "                if other_tag != 'O' and other_tag != tag:\n",
    "                    if valid_samples_count.get(other_tag, 0) + 1 > 0.1 * tag_count[other_tag]:\n",
    "                        satisfied = False\n",
    "                        break\n",
    "            if satisfied:\n",
    "                valid_samples.add(sample)\n",
    "                valid_samples_count[tag] = valid_samples_count.get(tag, 0) + 1\n",
    "                for other_tag in other_tags:\n",
    "                    if other_tag != 'O' and other_tag != tag:\n",
    "                        valid_samples_count[other_tag] = valid_samples_count.get(other_tag, 0) + 1\n",
    "print(len(valid_samples))\n",
    "print(valid_samples_count)\n",
    "print(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f31488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in valid_samples_count:\n",
    "    print(tag, ':', valid_samples_count[tag] / tag_count[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cbadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = set()\n",
    "test_samples_count = {}\n",
    "\n",
    "for tag in fine_classes:\n",
    "    samples = set(fine_dict[tag]).difference(train_samples.union(valid_samples))\n",
    "    test_samples_count[tag] = test_samples_count.get(tag, 0) + len(samples)\n",
    "    for sample in samples:\n",
    "        test_samples.add(sample)\n",
    "print(len(test_samples))\n",
    "print(test_samples_count)\n",
    "print(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in test_samples_count:\n",
    "    print(tag, ':', test_samples_count[tag] / tag_count[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510020fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', encoding='utf-8', mode='w') as f:\n",
    "    f.writelines('\\n\\n'.join(list(train_samples)))\n",
    "with open('dev.txt', encoding='utf-8', mode='w') as f:\n",
    "    f.writelines('\\n\\n'.join(list(valid_samples)))\n",
    "with open('test.txt', encoding='utf-8', mode='w') as f:\n",
    "    f.writelines('\\n\\n'.join(list(test_samples)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
