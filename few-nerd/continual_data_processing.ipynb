{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92edbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "parent_dir = 'supervised/supervised/'\n",
    "file_list = [os.path.join(parent_dir, file) for file in os.listdir(parent_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059e397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188239\n"
     ]
    }
   ],
   "source": [
    "raw_lines = []\n",
    "for file in file_list:\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines[-1] = lines[-1] + '\\n'\n",
    "    lines.extend(['\\n'])\n",
    "    lines = [line.strip() for line in lines]\n",
    "    raw_lines.extend(lines)\n",
    "\n",
    "all_sentences = []\n",
    "beg, end = 0, 0\n",
    "while end < len(raw_lines):\n",
    "        if raw_lines[end]:\n",
    "            end += 1\n",
    "        else:\n",
    "            sentence = raw_lines[beg:end]\n",
    "            all_sentences.append(sentence)\n",
    "            end += 1\n",
    "            beg = end\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91375b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154387\n",
      "23482\n"
     ]
    }
   ],
   "source": [
    "washed_sentences = set()\n",
    "no_entities_sentences = set()\n",
    "for sentence in all_sentences:\n",
    "    O_count = sum([1 if wt.endswith('\\tO') else 0 for wt in sentence])\n",
    "    if O_count != len(sentence):\n",
    "        washed_sentences.add('\\n'.join(sentence))\n",
    "    else:\n",
    "        no_entities_sentences.add('\\n'.join(sentence))\n",
    "print(len(washed_sentences))\n",
    "print(len(no_entities_sentences))\n",
    "all_sentences = list(washed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696db900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse-grained classes: {'person', 'building', 'event', 'other', 'product', 'location', 'art', 'organization'}; \n",
      "fine-grained classes: {'building-hospital', 'art-broadcastprogram', 'other-disease', 'building-restaurant', 'location-road,railway,highway,transit', 'other-currency', 'event-disaster', 'organization-education', 'other-chemicalthing', 'building-airport', 'event-other', 'art-other', 'location-park', 'other-language', 'product-food', 'location-GPE', 'other-educationaldegree', 'building-other', 'building-theater', 'product-ship', 'art-writtenart', 'organization-sportsteam', 'art-painting', 'other-livingthing', 'event-attack,battle,war,militaryconflict', 'product-weapon', 'building-hotel', 'organization-showorganization', 'location-other', 'organization-company', 'other-law', 'organization-politicalparty', 'product-other', 'location-island', 'location-mountain', 'product-software', 'other-award', 'person-artist,author', 'other-medical', 'person-actor', 'other-god', 'organization-government,governmentagency', 'location-bodiesofwater', 'art-film', 'event-sportsevent', 'event-election', 'art-music', 'person-athlete', 'person-scholar', 'person-soldier', 'product-car', 'other-biologything', 'product-game', 'organization-media,newspaper', 'person-other', 'event-protest', 'other-astronomything', 'organization-other', 'organization-religion', 'building-library', 'product-train', 'product-airplane', 'person-director', 'person-politician', 'building-sportsfacility', 'organization-sportsleague'}\n",
      "In\tO\n",
      "his\tO\n",
      "later\tO\n",
      "life\tO\n",
      "Coleman\tperson\n",
      "joined\tO\n",
      "the\tO\n",
      "Readjuster\torganization\n",
      "Party\torganization\n",
      ",\tO\n",
      "a\tO\n",
      "move\tO\n",
      "common\tO\n",
      "among\tO\n",
      "African\tlocation\n",
      "Americans\tlocation\n",
      "of\tO\n",
      "the\tO\n",
      "time\tO\n",
      ",\tO\n",
      "and\tO\n",
      "was\tO\n",
      "a\tO\n",
      "member\tO\n",
      "of\tO\n",
      "the\tO\n",
      "Republican\torganization\n",
      "Party\torganization\n",
      "of\tO\n",
      "Virginia\tlocation\n",
      "after\tO\n",
      "they\tO\n",
      "broke\tO\n",
      "from\tO\n",
      "the\tO\n",
      "Readjuster\torganization\n",
      "Party\torganization\n",
      "in\tO\n",
      "1884\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "coarse_classes = set()\n",
    "fine_classes = set()\n",
    "for k in range(len(all_sentences)):\n",
    "    wts = all_sentences[k].split('\\n')\n",
    "    for i in range(len(wts)):\n",
    "        word_tag = wts[i]\n",
    "        tokens = word_tag.split('\\t')\n",
    "        if tokens[1] != 'O':\n",
    "            if '/' in tokens[1]:\n",
    "                tokens[1] = tokens[1].replace('/', ',')\n",
    "                wts[i] = '\\t'.join(tokens)\n",
    "            fine_classes.add(tokens[1])\n",
    "            coarse_classes.add(tokens[1].split('-')[0])\n",
    "    all_sentences[k] = '\\n'.join(wts)\n",
    "print(f'coarse-grained classes: {coarse_classes}; \\nfine-grained classes: {fine_classes}')\n",
    "\n",
    "coarse_dict = {}\n",
    "fine_dict = {}\n",
    "\n",
    "coarse_sid_dict = {}\n",
    "fine_sid_dict = {}\n",
    "\n",
    "coarse_sentences = ['\\n'.join([wt.replace(wt.split('\\t')[1], wt.split('\\t')[1].split('-')[0]) for wt in sentence.split('\\n')]) for sentence in all_sentences]\n",
    "print(coarse_sentences[0])\n",
    "\n",
    "for coarse_class in coarse_classes:\n",
    "    sentences = []\n",
    "    sid_list = []\n",
    "    out_sid = 0\n",
    "    for sentence in coarse_sentences:\n",
    "        wts = sentence.split('\\n')\n",
    "        for word_tag in wts:\n",
    "            if word_tag.endswith(coarse_class):\n",
    "                sentences.append(sentence)\n",
    "                sid_list.append(out_sid)\n",
    "                break\n",
    "        out_sid += 1\n",
    "    coarse_dict[coarse_class] = sentences\n",
    "    coarse_sid_dict[coarse_class] = sid_list\n",
    "\n",
    "for fine_class in fine_classes:\n",
    "    sentences = []\n",
    "    sid_list = []\n",
    "    out_sid = 0\n",
    "    for sentence in all_sentences:\n",
    "        wts = sentence.split('\\n')\n",
    "        for word_tag in wts:\n",
    "            if word_tag.endswith(fine_class):\n",
    "                sentences.append(sentence)\n",
    "                sid_list.append(out_sid)\n",
    "                break\n",
    "        out_sid += 1\n",
    "    fine_dict[fine_class] = sentences\n",
    "    fine_sid_dict[fine_class] = sid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a70da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': 34843, 'building': 14757, 'event': 14452, 'other': 20353, 'product': 14738, 'location': 23014, 'art': 12676, 'organization': 19554}\n",
      "{'building-hospital': 1239, 'art-broadcastprogram': 1924, 'other-disease': 1959, 'building-restaurant': 770, 'location-road,railway,highway,transit': 3719, 'other-currency': 1978, 'event-disaster': 877, 'organization-education': 4152, 'other-chemicalthing': 2087, 'building-airport': 1280, 'event-other': 2452, 'art-other': 1591, 'location-park': 1557, 'other-language': 1927, 'product-food': 1044, 'location-GPE': 12243, 'other-educationaldegree': 1175, 'building-other': 4445, 'building-theater': 1612, 'product-ship': 1401, 'art-writtenart': 2180, 'organization-sportsteam': 3688, 'art-painting': 213, 'other-livingthing': 1908, 'event-attack,battle,war,militaryconflict': 2579, 'product-weapon': 1474, 'building-hotel': 1079, 'organization-showorganization': 1900, 'location-other': 3619, 'organization-company': 5214, 'other-law': 1776, 'organization-politicalparty': 2509, 'product-other': 3022, 'location-island': 1858, 'location-mountain': 1861, 'product-software': 1901, 'other-award': 2105, 'person-artist,author': 3076, 'other-medical': 1198, 'person-actor': 1755, 'other-god': 1247, 'organization-government,governmentagency': 3201, 'location-bodiesofwater': 2625, 'art-film': 1521, 'event-sportsevent': 3263, 'event-election': 719, 'art-music': 1955, 'person-athlete': 3369, 'person-scholar': 1973, 'person-soldier': 2053, 'product-car': 1773, 'other-biologything': 2665, 'product-game': 1319, 'organization-media,newspaper': 2189, 'person-other': 6053, 'event-protest': 675, 'other-astronomything': 1657, 'organization-other': 5649, 'organization-religion': 1986, 'building-library': 1272, 'product-train': 992, 'product-airplane': 2035, 'person-director': 1505, 'person-politician': 4075, 'building-sportsfacility': 1545, 'organization-sportsleague': 2724}\n"
     ]
    }
   ],
   "source": [
    "# remove duplicated sentences among different classes\n",
    "key_list = list(coarse_sid_dict.keys())\n",
    "for i in range(len(key_list)):\n",
    "    a_sids = set(coarse_sid_dict[key_list[i]])\n",
    "    for j in range(i + 1, len(key_list)):\n",
    "        b_sids = set(coarse_sid_dict[key_list[j]])\n",
    "        c = a_sids.intersection(b_sids)\n",
    "        m, n = len(a_sids), len(b_sids)\n",
    "        for sid in c:\n",
    "            removed_sentence = coarse_sentences[sid]\n",
    "            if m > n:\n",
    "                coarse_dict[key_list[i]].remove(removed_sentence)\n",
    "                coarse_sid_dict[key_list[i]].remove(sid)\n",
    "                a_sids.remove(sid)\n",
    "                m -= 1\n",
    "            else:\n",
    "                coarse_dict[key_list[j]].remove(removed_sentence)\n",
    "                coarse_sid_dict[key_list[j]].remove(sid)\n",
    "                b_sids.remove(sid)\n",
    "                n -= 1\n",
    "\n",
    "total_sentence_num = 0\n",
    "\n",
    "# remove duplicated sentences among different classes\n",
    "key_list = list(fine_sid_dict.keys())\n",
    "for i in range(len(key_list)):\n",
    "    a_sids = set(fine_sid_dict[key_list[i]])\n",
    "    for j in range(i + 1, len(key_list)):\n",
    "        b_sids = set(fine_sid_dict[key_list[j]])\n",
    "        c = a_sids.intersection(b_sids)\n",
    "        m, n = len(a_sids), len(b_sids)\n",
    "        for sid in c:\n",
    "            removed_sentence = all_sentences[sid]\n",
    "            if m > n:\n",
    "                fine_dict[key_list[i]].remove(removed_sentence)\n",
    "                fine_sid_dict[key_list[i]].remove(sid)\n",
    "                a_sids.remove(sid)\n",
    "                m -= 1\n",
    "            else:\n",
    "                fine_dict[key_list[j]].remove(removed_sentence)\n",
    "                fine_sid_dict[key_list[j]].remove(sid)\n",
    "                b_sids.remove(sid)\n",
    "                n -= 1\n",
    "\n",
    "coarse_tag_count, fine_tag_count = {}, {}\n",
    "for c in coarse_classes:\n",
    "    coarse_tag_count[c] = len(coarse_dict[c])\n",
    "for c in fine_classes:\n",
    "    fine_tag_count[c] = len(fine_dict[c])\n",
    "print(coarse_tag_count)\n",
    "print(fine_tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d230ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corse_distribution = {}\n",
    "for c in coarse_dict:\n",
    "    distribution = {}\n",
    "    for s in coarse_dict[c]:\n",
    "        t_set = set()\n",
    "        for w_t in s.split('\\n'):\n",
    "            t = w_t.split('\\t')[1] \n",
    "            if t != 'O':\n",
    "                t_set.add(t.split('-')[0])\n",
    "        for t in t_set:\n",
    "            distribution[t] = distribution.get(t, 0) + 1\n",
    "    corse_distribution[c] = distribution\n",
    "with open('./continual/coarse/distribution.json', encoding='utf-8', mode='w') as f:\n",
    "    json.dump(corse_distribution, f)\n",
    "\n",
    "fine_distribution = {}\n",
    "for c in fine_dict:\n",
    "    distribution = {}\n",
    "    for s in fine_dict[c]:\n",
    "        t_set = set()\n",
    "        for w_t in s.split('\\n'):\n",
    "            t = w_t.split('\\t')[1] \n",
    "            #\\ .split('-')[0]\n",
    "            if t != 'O':\n",
    "                t_set.add(t)\n",
    "        for t in t_set:\n",
    "            distribution[t] = distribution.get(t, 0) + 1\n",
    "    fine_distribution[c] = distribution\n",
    "\n",
    "with open('./continual/fine/distribution.json', encoding='utf-8', mode='w') as f:\n",
    "    json.dump(fine_distribution, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755eb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del washed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb464d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9eccd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_coarse_dict, no_fine_dict = {}, {}\n",
    "for coarse in coarse_classes:\n",
    "    sample_ids = coarse_sid_dict[coarse]\n",
    "    no_samples = []\n",
    "    for sample_id in sample_ids:\n",
    "        sample = coarse_sentences[sample_id]\n",
    "        wts = sample.split('\\n')\n",
    "        for i in range(len(wts)):\n",
    "            word_tag = wts[i].split('\\t')\n",
    "            if word_tag[1] != 'O' and word_tag[1] != coarse:\n",
    "                word_tag[1] = 'O'\n",
    "                wts[i] = '\\t'.join(word_tag)\n",
    "        no_samples.append('\\n'.join(wts))\n",
    "    no_coarse_dict[coarse] = no_samples\n",
    "for fine in fine_classes:\n",
    "    sample_ids = fine_sid_dict[fine]\n",
    "    no_samples = []\n",
    "    for sample_id in sample_ids:\n",
    "        sample = all_sentences[sample_id]\n",
    "        wts = sample.split('\\n')\n",
    "        for i in range(len(wts)):\n",
    "            word_tag = wts[i].split('\\t')\n",
    "            if word_tag[1] != 'O' and word_tag[1] != fine:\n",
    "                word_tag[1] = 'O'\n",
    "                wts[i] = '\\t'.join(word_tag)\n",
    "        no_samples.append('\\n'.join(wts))\n",
    "    no_fine_dict[fine] = no_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4683e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person : train : 24391; valid: 3485; test : 6967; total : 34843; ratio : 0.7000258301523979\n",
      "building : train : 10330; valid: 1476; test : 2951; total : 14757; ratio : 0.7000067764450769\n",
      "event : train : 10117; valid: 1446; test : 2889; total : 14452; ratio : 0.7000415167450872\n",
      "other : train : 14248; valid: 2036; test : 4069; total : 20353; ratio : 0.7000442195253771\n",
      "product : train : 10317; valid: 1474; test : 2947; total : 14738; ratio : 0.7000271407246573\n",
      "location : train : 16110; valid: 2302; test : 4602; total : 23014; ratio : 0.7000086903623881\n",
      "art : train : 8874; valid: 1268; test : 2534; total : 12676; ratio : 0.7000631113916062\n",
      "organization : train : 13688; valid: 1956; test : 3910; total : 19554; ratio : 0.7000102280863251\n",
      "================================================\n",
      "building-hospital : train : 868; valid: 124; test : 247; total : 1239; ratio : 0.7005649717514124\n",
      "art-broadcastprogram : train : 1347; valid: 193; test : 384; total : 1924; ratio : 0.7001039501039501\n",
      "other-disease : train : 1372; valid: 196; test : 391; total : 1959; ratio : 0.700357325165901\n",
      "building-restaurant : train : 539; valid: 77; test : 154; total : 770; ratio : 0.7\n",
      "location-road,railway,highway,transit : train : 2604; valid: 372; test : 743; total : 3719; ratio : 0.7001882226404947\n",
      "other-currency : train : 1385; valid: 198; test : 395; total : 1978; ratio : 0.7002022244691608\n",
      "event-disaster : train : 614; valid: 88; test : 175; total : 877; ratio : 0.7001140250855188\n",
      "organization-education : train : 2907; valid: 416; test : 829; total : 4152; ratio : 0.7001445086705202\n",
      "other-chemicalthing : train : 1461; valid: 209; test : 417; total : 2087; ratio : 0.7000479156684236\n",
      "building-airport : train : 896; valid: 128; test : 256; total : 1280; ratio : 0.7\n",
      "event-other : train : 1717; valid: 246; test : 489; total : 2452; ratio : 0.7002446982055465\n",
      "art-other : train : 1114; valid: 160; test : 317; total : 1591; ratio : 0.700188560653677\n",
      "location-park : train : 1090; valid: 156; test : 311; total : 1557; ratio : 0.7000642260757868\n",
      "other-language : train : 1349; valid: 193; test : 385; total : 1927; ratio : 0.7000518941359626\n",
      "product-food : train : 731; valid: 105; test : 208; total : 1044; ratio : 0.7001915708812261\n",
      "location-GPE : train : 8571; valid: 1225; test : 2447; total : 12243; ratio : 0.7000735113942661\n",
      "other-educationaldegree : train : 823; valid: 118; test : 234; total : 1175; ratio : 0.7004255319148937\n",
      "building-other : train : 3112; valid: 445; test : 888; total : 4445; ratio : 0.7001124859392576\n",
      "building-theater : train : 1129; valid: 162; test : 321; total : 1612; ratio : 0.7003722084367245\n",
      "product-ship : train : 981; valid: 141; test : 279; total : 1401; ratio : 0.7002141327623126\n",
      "art-writtenart : train : 1526; valid: 218; test : 436; total : 2180; ratio : 0.7\n",
      "organization-sportsteam : train : 2582; valid: 369; test : 737; total : 3688; ratio : 0.7001084598698482\n",
      "art-painting : train : 150; valid: 22; test : 41; total : 213; ratio : 0.704225352112676\n",
      "other-livingthing : train : 1336; valid: 191; test : 381; total : 1908; ratio : 0.70020964360587\n",
      "event-attack,battle,war,militaryconflict : train : 1806; valid: 258; test : 515; total : 2579; ratio : 0.700271423032183\n",
      "product-weapon : train : 1032; valid: 148; test : 294; total : 1474; ratio : 0.7001356852103121\n",
      "building-hotel : train : 756; valid: 108; test : 215; total : 1079; ratio : 0.70064874884152\n",
      "organization-showorganization : train : 1330; valid: 190; test : 380; total : 1900; ratio : 0.7\n",
      "location-other : train : 2534; valid: 362; test : 723; total : 3619; ratio : 0.7001934235976789\n",
      "organization-company : train : 3650; valid: 522; test : 1042; total : 5214; ratio : 0.7000383582662064\n",
      "other-law : train : 1244; valid: 178; test : 354; total : 1776; ratio : 0.7004504504504504\n",
      "organization-politicalparty : train : 1757; valid: 251; test : 501; total : 2509; ratio : 0.7002789956157832\n",
      "product-other : train : 2116; valid: 303; test : 603; total : 3022; ratio : 0.700198544010589\n",
      "location-island : train : 1301; valid: 186; test : 371; total : 1858; ratio : 0.7002152852529602\n",
      "location-mountain : train : 1303; valid: 187; test : 371; total : 1861; ratio : 0.7001612036539495\n",
      "product-software : train : 1331; valid: 191; test : 379; total : 1901; ratio : 0.7001578116780641\n",
      "other-award : train : 1474; valid: 211; test : 420; total : 2105; ratio : 0.7002375296912114\n",
      "person-artist,author : train : 2154; valid: 308; test : 614; total : 3076; ratio : 0.7002600780234071\n",
      "other-medical : train : 839; valid: 120; test : 239; total : 1198; ratio : 0.7003338898163606\n",
      "person-actor : train : 1229; valid: 176; test : 350; total : 1755; ratio : 0.7002849002849003\n",
      "other-god : train : 873; valid: 125; test : 249; total : 1247; ratio : 0.7000801924619086\n",
      "organization-government,governmentagency : train : 2241; valid: 321; test : 639; total : 3201; ratio : 0.7000937207122774\n",
      "location-bodiesofwater : train : 1838; valid: 263; test : 524; total : 2625; ratio : 0.7001904761904761\n",
      "art-film : train : 1065; valid: 153; test : 303; total : 1521; ratio : 0.7001972386587771\n",
      "event-sportsevent : train : 2285; valid: 327; test : 651; total : 3263; ratio : 0.7002758197977321\n",
      "event-election : train : 504; valid: 72; test : 143; total : 719; ratio : 0.7009735744089013\n",
      "art-music : train : 1369; valid: 196; test : 390; total : 1955; ratio : 0.7002557544757033\n",
      "person-athlete : train : 2359; valid: 337; test : 673; total : 3369; ratio : 0.7002077767883645\n",
      "person-scholar : train : 1382; valid: 198; test : 393; total : 1973; ratio : 0.7004561581348201\n",
      "person-soldier : train : 1438; valid: 206; test : 409; total : 2053; ratio : 0.7004383828543594\n",
      "product-car : train : 1242; valid: 178; test : 353; total : 1773; ratio : 0.700507614213198\n",
      "other-biologything : train : 1866; valid: 267; test : 532; total : 2665; ratio : 0.700187617260788\n",
      "product-game : train : 924; valid: 132; test : 263; total : 1319; ratio : 0.7005307050796058\n",
      "organization-media,newspaper : train : 1533; valid: 219; test : 437; total : 2189; ratio : 0.7003197807217908\n",
      "person-other : train : 4238; valid: 606; test : 1209; total : 6053; ratio : 0.7001486866016852\n",
      "event-protest : train : 473; valid: 68; test : 134; total : 675; ratio : 0.7007407407407408\n",
      "other-astronomything : train : 1160; valid: 166; test : 331; total : 1657; ratio : 0.700060350030175\n",
      "organization-other : train : 3955; valid: 565; test : 1129; total : 5649; ratio : 0.7001239157372986\n",
      "organization-religion : train : 1391; valid: 199; test : 396; total : 1986; ratio : 0.7004028197381672\n",
      "building-library : train : 891; valid: 128; test : 253; total : 1272; ratio : 0.7004716981132075\n",
      "product-train : train : 695; valid: 100; test : 197; total : 992; ratio : 0.7006048387096774\n",
      "product-airplane : train : 1425; valid: 204; test : 406; total : 2035; ratio : 0.7002457002457002\n",
      "person-director : train : 1054; valid: 151; test : 300; total : 1505; ratio : 0.7003322259136212\n",
      "person-politician : train : 2853; valid: 408; test : 814; total : 4075; ratio : 0.7001226993865031\n",
      "building-sportsfacility : train : 1082; valid: 155; test : 308; total : 1545; ratio : 0.7003236245954693\n",
      "organization-sportsleague : train : 1907; valid: 273; test : 544; total : 2724; ratio : 0.7000734214390602\n"
     ]
    }
   ],
   "source": [
    "no_coarse_train_samples, no_coarse_valid_samples, no_coarse_test_samples = {}, {}, {}\n",
    "# no_coarse_train_indices, no_coarse_valid_indices, no_coarse_test_indices = {}, {}, {}\n",
    "no_fine_train_samples, no_fine_valid_samples, no_fine_test_samples = {}, {}, {}\n",
    "# no_fine_train_indices, no_fine_valid_indices, no_fine_test_indices = {}, {}, {}\n",
    "\n",
    "coarse_train_samples, coarse_valid_samples, coarse_test_samples = {}, {}, {}\n",
    "# coarse_train_indices, coarse_valid_indices, coarse_test_indices = {}, {}, {}\n",
    "fine_train_samples, fine_valid_samples, fine_test_samples = {}, {}, {}\n",
    "# fine_train_indices, fine_valid_indices, fine_test_indices = {}, {}, {}\n",
    "\n",
    "\n",
    "for c in coarse_classes:\n",
    "    sample_ids = coarse_sid_dict[c]\n",
    "    train_sample_id_indices, valid_sample_id_indices = set(), set()\n",
    "    train_count, valid_count = 0, 0\n",
    "    while train_count < 0.7 * coarse_tag_count[c]:\n",
    "        sample_id_idx = random.choice(range(len(sample_ids)))\n",
    "        if sample_id_idx not in train_sample_id_indices:\n",
    "            train_sample_id_indices.add(sample_id_idx)\n",
    "            train_count += 1\n",
    "    while valid_count < 0.1 * coarse_tag_count[c]:\n",
    "        sample_id_idx = random.choice(range(len(sample_ids)))\n",
    "        if sample_id_idx not in train_sample_id_indices and sample_id_idx not in valid_sample_id_indices:\n",
    "            valid_sample_id_indices.add(sample_id_idx)\n",
    "            valid_count += 1\n",
    "    test_sample_id_indices = set(range(len(sample_ids))).difference(train_sample_id_indices.union(valid_sample_id_indices))\n",
    "    no_coarse_train_samples[c] = [no_coarse_dict[c][sample_id_idx] for sample_id_idx in train_sample_id_indices]\n",
    "    coarse_train_samples[c] = [coarse_dict[c][sample_id_idx] for sample_id_idx in train_sample_id_indices]\n",
    "    no_coarse_valid_samples[c] = [no_coarse_dict[c][sample_id_idx] for sample_id_idx in valid_sample_id_indices]\n",
    "    coarse_valid_samples[c] = [coarse_dict[c][sample_id_idx] for sample_id_idx in valid_sample_id_indices]\n",
    "    no_coarse_test_samples[c] = [no_coarse_dict[c][sample_id_idx] for sample_id_idx in test_sample_id_indices]\n",
    "    coarse_test_samples[c] = [coarse_dict[c][sample_id_idx] for sample_id_idx in test_sample_id_indices]\n",
    "    \n",
    "#     no_coarse_train_indices[c] = train_sample_id_indices\n",
    "#     no_coarse_valid_indices[c] = valid_sample_id_indices\n",
    "#     no_coarse_test_indices[c] = test_sample_id_indices\n",
    "for c in coarse_tag_count:\n",
    "    print(f'{c} : train : {len(no_coarse_train_samples[c])}; valid: {len(no_coarse_valid_samples[c])}; test : {len(no_coarse_test_samples[c])}; total : {coarse_tag_count[c]}; ratio : {len(no_coarse_train_samples[c]) / coarse_tag_count[c]}')\n",
    "\n",
    "    \n",
    "print('================================================')\n",
    "\n",
    "    \n",
    "for c in fine_classes:\n",
    "    sample_ids = fine_sid_dict[c]\n",
    "    train_sample_id_indices, valid_sample_id_indices = set(), set()\n",
    "    train_count, valid_count = 0, 0\n",
    "    while train_count < 0.7 * fine_tag_count[c]:\n",
    "        sample_id_idx = random.choice(range(len(sample_ids)))\n",
    "        if sample_id_idx not in train_sample_id_indices:\n",
    "            train_sample_id_indices.add(sample_id_idx)\n",
    "            train_count += 1\n",
    "    while valid_count < 0.1 * fine_tag_count[c]:\n",
    "        sample_id_idx = random.choice(range(len(sample_ids)))\n",
    "        if sample_id_idx not in train_sample_id_indices and sample_id_idx not in valid_sample_id_indices:\n",
    "            valid_sample_id_indices.add(sample_id_idx)\n",
    "            valid_count += 1\n",
    "    test_sample_id_indices = set(range(len(sample_ids))).difference(train_sample_id_indices.union(valid_sample_id_indices))\n",
    "    no_fine_train_samples[c] = [no_fine_dict[c][sample_id_idx] for sample_id_idx in train_sample_id_indices]\n",
    "    no_fine_valid_samples[c] = [no_fine_dict[c][sample_id_idx] for sample_id_idx in valid_sample_id_indices]\n",
    "    no_fine_test_samples[c] = [no_fine_dict[c][sample_id_idx] for sample_id_idx in test_sample_id_indices]\n",
    "    \n",
    "    fine_train_samples[c] = [fine_dict[c][sample_id_idx] for sample_id_idx in train_sample_id_indices]\n",
    "    fine_valid_samples[c] = [fine_dict[c][sample_id_idx] for sample_id_idx in valid_sample_id_indices]\n",
    "    fine_test_samples[c] = [fine_dict[c][sample_id_idx] for sample_id_idx in test_sample_id_indices]\n",
    "#     no_fine_train_indices[c] = train_sample_id_indices\n",
    "#     no_fine_valid_indices[c] = valid_sample_id_indices\n",
    "#     no_fine_test_indices[c] = test_sample_id_indices\n",
    "for c in fine_tag_count:\n",
    "    print(f'{c} : train : {len(no_fine_train_samples[c])}; valid: {len(no_fine_valid_samples[c])}; test : {len(no_fine_test_samples[c])}; total : {fine_tag_count[c]}; ratio : {len(no_fine_train_samples[c]) / fine_tag_count[c]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05535b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train_samples, coarse_test_samples = {}, {}\n",
    "fine_train_samples, fine_test_samples = {}, {}\n",
    "for c in coarse_classes:\n",
    "    train_samples = [coarse_dict[c][i] for i in no_coarse_train_indices[c]]\n",
    "    coarse_train_samples[c] = train_samples\n",
    "for c in coarse_tag_count:\n",
    "    print(f'{c} : train : {len(coarse_train_samples[c])}; total : {coarse_tag_count[c]}; ratio : {len(coarse_train_samples[c]) / coarse_tag_count[c]}')\n",
    "# remained_samples = set()\n",
    "for c in fine_classes:\n",
    "    train_samples = [fine_dict[c][i] for i in no_fine_train_indices[c]]\n",
    "    fine_train_samples[c] = train_samples\n",
    "#     remained_samples = remained_samples.union(set([fine_dict[c][i] for i in no_fine_test_indices[c]]))\n",
    "for c in fine_tag_count:\n",
    "    print(f'{c} : train : {len(fine_train_samples[c])}; total : {fine_tag_count[c]}; ratio : {len(fine_train_samples[c]) / fine_tag_count[c]}')\n",
    "\n",
    "# for c in fine_tag_count:\n",
    "#     train_samples = fine_train_samples[c]\n",
    "#     train_dist = {}\n",
    "#     for train_sample in train_samples:\n",
    "#         tags = set(wt .split('\\t')[1] for wt in train_sample.split('\\n'))\n",
    "#         tags.remove('O')\n",
    "#         for tag in tags:\n",
    "#             train_dist[tag] = train_dist.get(tag, 0) + 1\n",
    "#     test_samples, test_tag_count = set(), {}\n",
    "#     for train_tag in train_dist:\n",
    "#         visited = set()\n",
    "#         while test_tag_count.get(train_tag, 0) < (0.25 * train_dist[train_tag] if train_dist[train_tag] > 4 else 1):\n",
    "#             test_sample = random.choice(list(remained_samples))\n",
    "#             visited.add(test_sample)\n",
    "#             if visited == remained_samples:\n",
    "#                 print('No satisfied sample. Exit search.')\n",
    "#                 break\n",
    "#             if test_sample not in test_samples and test_sample not in train_samples:\n",
    "#                 satisfied = True\n",
    "#                 tags = set([wt.split('\\t')[1] for wt in test_sample.split('\\n')])\n",
    "#                 tags.remove('O')\n",
    "#                 if tags.issubset(train_dist.keys()):\n",
    "#                     for t in tags:\n",
    "#                         if test_tag_count.get(t, 0) + 1 > (0.25 * train_dist[t] if train_dist[t] > 4 else 1):\n",
    "#                             satisfied = False\n",
    "#                             break\n",
    "#                 else:\n",
    "#                     satisfied = False\n",
    "#                 if satisfied:\n",
    "#                     test_samples.add(test_sample)\n",
    "#                     remained_samples.remove(test_sample)\n",
    "#                     visited.remove(test_sample)\n",
    "#                     test_tag_count[train_tag] = test_tag_count.get(train_tag, 0) + 1\n",
    "#                     for t in tags:\n",
    "#                         if t != 'O' and t != train_tag:\n",
    "#                             test_tag_count[t] = test_tag_count.get(t, 0) + 1\n",
    "#     fine_test_samples[c] = test_samples\n",
    "#     print(f'{c} test samples found!')\n",
    "# for c in tag_count:\n",
    "#     print(f'{c} : test : {len(fine_test_samples[c])}; total : {fine_tag_count[c]}; ratio : {len(fine_test_samples[c]) / fine_tag_count[c]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7deb2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(parent_dir, samples_dict, mode):\n",
    "    for fname in samples_dict:\n",
    "        path = os.path.join(parent_dir, fname)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(os.path.join(path, mode+'.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.writelines('\\n\\n'.join(samples_dict[fname]))\n",
    "save_file('./continual/coarse/disjoint', no_coarse_train_samples, mode='train')\n",
    "save_file('./continual/coarse/disjoint', no_coarse_valid_samples, mode='dev')\n",
    "save_file('./continual/coarse/disjoint', no_coarse_test_samples, mode='test')\n",
    "save_file('./continual/fine/disjoint', no_fine_train_samples, mode='train')\n",
    "save_file('./continual/fine/disjoint', no_fine_valid_samples, mode='dev')\n",
    "save_file('./continual/fine/disjoint', no_fine_test_samples, mode='test')\n",
    "\n",
    "save_file('./continual/coarse/joint', coarse_train_samples, mode='train')\n",
    "save_file('./continual/coarse/joint', coarse_valid_samples, mode='dev')\n",
    "save_file('./continual/coarse/joint', coarse_test_samples, mode='test')\n",
    "save_file('./continual/fine/joint', fine_train_samples, mode='train')\n",
    "save_file('./continual/fine/joint', fine_valid_samples, mode='dev')\n",
    "save_file('./continual/fine/joint', fine_test_samples, mode='test')\n",
    "\n",
    "save_file('./continual/fine/joint', fine_test_samples, mode='test')\n",
    "\n",
    "# save_file('./continual/fine/overlapping', fine_test_samples, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('no_entities.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.writelines('\\n\\n'.join(no_entities_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e797c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
