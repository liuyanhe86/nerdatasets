{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1d4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68f5ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9446\n",
      "5207\n"
     ]
    }
   ],
   "source": [
    "# 读取所有的原始样本\n",
    "all_samples, no_entities_samples = {}, {}\n",
    "temporal_files, skewed_files = os.listdir('./temporal_splits/'), os.listdir('./skewed_splits/')\n",
    "for file in temporal_files:\n",
    "    with open('./temporal_splits/' + file, encoding='utf-8') as f:\n",
    "        for sample in json.load(f):\n",
    "            tags = sample['tag_sequence'].split(' ')\n",
    "            if tags.count('O') < len(tags):\n",
    "                all_samples[sample['sentence']] = sample['tag_sequence'].replace('B-', '').replace('I-','')\n",
    "            else:\n",
    "                no_entities_samples[sample['sentence']] = sample['tag_sequence']\n",
    "for file in skewed_files:\n",
    "    with open('./skewed_splits/' + file, encoding='utf-8') as f:\n",
    "        for sample in json.load(f):\n",
    "            tags = sample['tag_sequence'].split(' ')\n",
    "            if tags.count('O') < len(tags):\n",
    "                all_samples[sample['sentence']] = sample['tag_sequence'].replace('B-', '').replace('I-','')\n",
    "            else:\n",
    "                no_entities_samples[sample['sentence']] = sample['tag_sequence']\n",
    "print(len(all_samples))\n",
    "print(len(no_entities_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d825a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14707\n",
      "9446\n"
     ]
    }
   ],
   "source": [
    "tag_set = set()\n",
    "for sentence in all_samples:\n",
    "    tags = all_samples[sentence].split()\n",
    "    for tag in tags:\n",
    "        if tag != 'O':\n",
    "            tag_set.add(tag)\n",
    "sample_dict = {}\n",
    "for tag in tag_set:\n",
    "    samples = []\n",
    "    for sentence in all_samples:\n",
    "        tags = all_samples[sentence].split()\n",
    "        if tag in tags:\n",
    "            tokens = sentence.split()\n",
    "            token_tag = []\n",
    "            for i in range(len(tags)):\n",
    "                token_tag.append(tokens[i] + '\\t' +tags[i])\n",
    "            samples.append('\\n'.join(token_tag))\n",
    "    sample_dict[tag] = samples\n",
    "count = 0\n",
    "for tag in sample_dict:\n",
    "    count += len(sample_dict[tag])\n",
    "print(count)\n",
    "count = 0\n",
    "\n",
    "\n",
    "tag_list = list(tag_set)\n",
    "for i in range(len(tag_list)):\n",
    "    ti_samples = set(sample_dict[tag_list[i]])\n",
    "    for j in range(i + 1, len(tag_list)):\n",
    "        tj_samples = set(sample_dict[tag_list[j]])\n",
    "        inter = ti_samples.intersection(tj_samples)\n",
    "        c = len(inter)\n",
    "        if c > 0:\n",
    "            a, b = len(ti_samples), len(tj_samples)\n",
    "            for sample in inter:\n",
    "                if a > b:\n",
    "                    ti_samples.remove(sample)\n",
    "                    a -= 1\n",
    "                else:\n",
    "                    tj_samples.remove(sample)\n",
    "                    b -= 1\n",
    "            sample_dict[tag_list[i]] = list(ti_samples)\n",
    "            sample_dict[tag_list[j]] = list(tj_samples)\n",
    "for tag in sample_dict:\n",
    "    count += len(sample_dict[tag])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691fd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Function_Name', 'User_Name', 'Organization', 'Website', 'Error_Name', 'Licence', 'Output_Block', 'Algorithm', 'Keyboard_IP'}\n",
      "{'Version': 260, 'Data_Type': 429, 'Data_Structure': 621, 'Function_Name': 188, 'Library': 641, 'Code_Block': 613, 'Application': 744, 'User_Interface_Element': 667, 'Organization': 45, 'File_Type': 416, 'HTML_XML_Tag': 232, 'Error_Name': 104, 'Device': 277, 'Class_Name': 283, 'File_Name': 425, 'User_Name': 131, 'Variable_Name': 498, 'Algorithm': 60, 'Library_Function': 498, 'Library_Variable': 312, 'Language': 472, 'Value': 457, 'Licence': 1, 'Operating_System': 251, 'Output_Block': 84, 'Website': 119, 'Library_Class': 574, 'Keyboard_IP': 44}\n"
     ]
    }
   ],
   "source": [
    "few_sample_tags = set()\n",
    "tag_count = {}\n",
    "for tag in tag_set:\n",
    "    tag_count[tag] = len(sample_dict[tag])\n",
    "    if tag_count[tag] < 200:\n",
    "        few_sample_tags.add(tag)\n",
    "print(few_sample_tags)\n",
    "print(tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb927066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8670\n"
     ]
    }
   ],
   "source": [
    "for tag in few_sample_tags:\n",
    "    tag_set.remove(tag)\n",
    "    tag_count.pop(tag)\n",
    "    sample_dict.pop(tag)\n",
    "for few_sample_tag in few_sample_tags:\n",
    "    for tag in sample_dict:\n",
    "        clear_sentences = []\n",
    "        for sentence in sample_dict[tag]:\n",
    "            wts = sentence.split('\\n')\n",
    "            for i in range(len(wts)):\n",
    "                w_t = wts[i].split('\\t')\n",
    "                w_t[1] = w_t[1].replace(few_sample_tag, 'O')\n",
    "                wts[i] = '\\t'.join(w_t)\n",
    "            clear_sentences.append('\\n'.join(wts))\n",
    "        sample_dict[tag] = clear_sentences\n",
    "count = 0\n",
    "for tag in tag_count:\n",
    "    count += tag_count[tag]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65619060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version : 260\n",
      "Data_Type : 429\n",
      "Data_Structure : 621\n",
      "Library : 641\n",
      "Code_Block : 613\n",
      "Application : 744\n",
      "User_Interface_Element : 667\n",
      "File_Type : 416\n",
      "HTML_XML_Tag : 232\n",
      "Device : 277\n",
      "Class_Name : 283\n",
      "File_Name : 425\n",
      "Variable_Name : 498\n",
      "Library_Function : 498\n",
      "Library_Variable : 312\n",
      "Language : 472\n",
      "Value : 457\n",
      "Operating_System : 251\n",
      "Library_Class : 574\n"
     ]
    }
   ],
   "source": [
    "for entity_type in sample_dict:\n",
    "    print(entity_type,':',len(sample_dict[entity_type]))\n",
    "distribution = {}\n",
    "for entity_type in sample_dict:\n",
    "    samples = sample_dict[entity_type]\n",
    "    type_dict = {}\n",
    "    for sample in samples:\n",
    "        token_tags = sample.split('\\n')\n",
    "        tag_set = set()\n",
    "        for token_tag in token_tags:\n",
    "            tag = token_tag.split('\\t')[1]\n",
    "            if tag != 'O':\n",
    "                tag_set.add(tag)\n",
    "        for tag in tag_set:\n",
    "            type_dict[tag] = type_dict.get(tag, 0) + 1\n",
    "    distribution[entity_type] = type_dict\n",
    "with open('./continual/distribution.json', encoding='utf-8', mode='w') as f:\n",
    "    json.dump(distribution, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23e0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "no_dir = './continual/disjoint/'\n",
    "o_dir = './continual/joint/'\n",
    "for entity_type in sample_dict:\n",
    "    no_path = os.path.join(no_dir, entity_type)\n",
    "    o_path = os.path.join(o_dir, entity_type)\n",
    "    if not os.path.exists(no_path):\n",
    "        os.makedirs(no_path)\n",
    "    if not os.path.exists(o_path):\n",
    "        os.makedirs(o_path)\n",
    "    samples = sample_dict[entity_type]\n",
    "    train_samples, valid_samples = set(), set()\n",
    "    while len(train_samples) < 0.7 * len(samples):\n",
    "        sample = random.choice(samples)\n",
    "        if sample not in train_samples:\n",
    "            train_samples.add(sample)\n",
    "    while len(valid_samples) < 0.1 * len(samples):\n",
    "        sample = random.choice(samples)\n",
    "        if sample not in train_samples and sample not in valid_samples:\n",
    "            valid_samples.add(sample)\n",
    "    test_samples = set(samples).difference(train_samples.union(valid_samples))\n",
    "    with open(os.path.join(o_path, 'train.txt'), encoding='utf-8', mode='w') as f:\n",
    "        f.writelines('\\n\\n'.join(train_samples))\n",
    "    with open(os.path.join(no_path, 'train.txt'), encoding='utf-8', mode='w') as f:\n",
    "        samples = list(train_samples)\n",
    "        for i in range(len(samples)):\n",
    "            token_tags = samples[i].split('\\n')\n",
    "            for j in range(len(token_tags)):\n",
    "                tag = token_tags[j].split('\\t')[1]\n",
    "                if tag != 'O' and tag != entity_type:\n",
    "                    token_tags[j] = token_tags[j].replace(tag, 'O')\n",
    "            samples[i] = '\\n'.join(token_tags)\n",
    "        f.writelines('\\n\\n'.join(samples))\n",
    "    with open(os.path.join(o_path, 'dev.txt'), encoding='utf-8', mode='w') as f:\n",
    "        f.writelines('\\n\\n'.join(valid_samples))\n",
    "    with open(os.path.join(no_path, 'dev.txt'), encoding='utf-8', mode='w') as f:\n",
    "        samples = list(valid_samples)\n",
    "        for i in range(len(samples)):\n",
    "            token_tags = samples[i].split('\\n')\n",
    "            for j in range(len(token_tags)):\n",
    "                tag = token_tags[j].split('\\t')[1]\n",
    "                if tag != 'O' and tag != entity_type:\n",
    "                    token_tags[j] = token_tags[j].replace(tag, 'O')\n",
    "            samples[i] = '\\n'.join(token_tags)\n",
    "        f.writelines('\\n\\n'.join(samples))\n",
    "    with open(os.path.join(o_path, 'test.txt'), encoding='utf-8', mode='w') as f:\n",
    "        f.writelines('\\n\\n'.join(test_samples))\n",
    "    with open(os.path.join(no_path, 'test.txt'), encoding='utf-8', mode='w') as f:\n",
    "        samples = list(test_samples)\n",
    "        for i in range(len(samples)):\n",
    "            token_tags = samples[i].split('\\n')\n",
    "            for j in range(len(token_tags)):\n",
    "                tag = token_tags[j].split('\\t')[1]\n",
    "                if tag != 'O' and tag != entity_type:\n",
    "                    token_tags[j] = token_tags[j].replace(tag, 'O')\n",
    "            samples[i] = '\\n'.join(token_tags)\n",
    "        f.writelines('\\n\\n'.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4d799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_entities_samples_list = []\n",
    "for sentence in no_entities_samples:\n",
    "    tags = no_entities_samples[sentence].split()\n",
    "    tokens = sentence.split()\n",
    "    token_tag = []\n",
    "    for i in range(len(tags)):\n",
    "        token_tag.append(tokens[i] + '\\t' +tags[i])\n",
    "    no_entities_samples_list.append('\\n'.join(token_tag))\n",
    "with open('no_entities.txt', encoding='utf-8', mode='w') as f:\n",
    "    f.writelines('\\n\\n'.join(no_entities_samples_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87b191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
